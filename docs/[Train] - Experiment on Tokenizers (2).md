# [Train] Experiment on Tokenizers (2)

* Purpose: To explore how different tokenizers process Khmer text and see which we should use for token evaluation.
* Outcome: PrahokBART Tokenizer is selected as our baseline tokenizer because:
    * Unicode normalization and Khmer word segmentation: According to its research paper, PrahokBART applies Khmer Unicode normalization and Khmer word segmentation (Khmer-NLTK), ensuring the text is cleaned and that functional spaces are treated correctly for Khmer data (Kaing et al., 2025). This step is crucial because Khmer has no explicit word boundaries and is prone to Unicode inconsistencies, which can negatively impact model performance if left untreated (Buoy et al., 2021; Hosken et al., 2022).
    * Better for generation: PrahokBART was evaluated on Khmer machine translation, summarization, and headline generation tasks, and consistently outperformed multilingual baselines such as mBART50. Moreover, it is well-suited for generative tasks that align with our Khmer QA dataset format (Kaing et al., 2025). These results are significant since mBART50 is a widely used and strong multilingual baseline (Tang et al., 2020).
    * Architecture – Unigram SentencePiece: PrahokBART uses Unigram subword tokenization with SentencePiece, which is particularly effective for non-space-delimited languages like Khmer. This architecture produces linguistically motivated tokens and achieves better functional space generation compared to raw Unigram or BPE approaches (Kaing et al., 2025). The design choice is supported by the original SentencePiece paper, which highlights that the Unigram model is well-suited for languages without explicit word boundaries (Kudo, 2018).
* Reference:
    * Buoy, R., Kaing, H., & Sovann, S. (2021). Khmer word segmentation using BiLSTM-CRF with syllable-based features. Proceedings of the 18th International Conference on Natural Language Processing (ICON).
    * Hosken, M., Bird, S., & Simons, G. (2022). Challenges in processing Southeast Asian scripts: Khmer and beyond. Language Resources and Evaluation, 56(2), 345–360.
    * Kaing, H., Dabre, R., Song, H., Tran, V.-H., Tanaka, H., & Utiyama, M. (2025). PrahokBART: A pre-trained sequence-to-sequence model for Khmer natural language generation. In Proceedings of the 31st International Conference on Computational Linguistics (COLING 2025) (pp. 1309–1322). Association for Computational Linguistics.
    * Kudo, T. (2018). Subword regularization: Improving neural network translation models with multiple subword candidates. In Proceedings of ACL 2018 (pp. 66–75). Association for Computational Linguistics.
    * Tang, Y., Tran, C., Li, X., Chen, P. J., Goyal, N., Chaudhary, V., … & Fan, A. (2020). Multilingual translation with extensible multilingual pretraining and fine-tuning. arXiv preprint arXiv:2008.00401.
